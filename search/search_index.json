{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Astronomer Telescope is a tool to observe distant (or local!) Airflow installations,   and help you better understand your deployments. </p>"},{"location":"#what-is-it","title":"What is it?","text":"<p>It is a CLI that runs on your workstation and accesses remote Airflows to collect a common set of data.</p> <p>Optionally, it can be installed and run as an Airflow plugin via Starship.</p> <p>Telescope has been purpose-built to help administrators understand their Airflow installations and provide metadata to assist with migrations.</p> <p>Main features</p> <ul> <li>Analyze your Airflow deployment and execution environment to provide a snapshot of all configurations.</li> <li>Summarizes Airflow-specific settings (variables, connections, pools, etc.)</li> <li>Generates a report of runtime configurations (airflow.cfg)</li> <li>Generates a DAGs report including code quality metrics, execution statistics and more.</li> <li>Can be run on most deployment environments (Docker, Kubernetes, SSH Remote) and Airflow versions.</li> <li>Security and anonymity are built-in.</li> <li>Kubernetes and airflow.cfg sensitive values are redacted.</li> <li>Individual user information and secrets are never accessed.</li> <li>Reports can be parameterized to obfuscate DAG IDs and filenames.</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install","title":"Install","text":"<p>Install from pip <pre><code>python -m pip install astronomer-telescope\n</code></pre> or download a binary.</p>"},{"location":"#usage","title":"Usage","text":"<p>For more information, see requirements or usage</p>"},{"location":"#kubernetes-autodiscovery-assessment-mode","title":"Kubernetes Autodiscovery Assessment Mode","text":"<p>This will work if the Airflow instances are in Kubernetes and were deployed with one of the major Helm charts ( <code>component=scheduler</code> is used to identify the schedulers).</p> <p>It will use Helm to analyze the installation, and connect to the Airflow schedulers to gather metadata.</p> <p><pre><code>telescope --kubernetes --organization-name &lt;My Organization&gt;\n</code></pre> This will produce a file ending in <code>*.data.json</code> - which is an data payload that can be sent to Astronomer for further processing and detailed analysis.</p>"},{"location":"#ssh-assessment-mode","title":"SSH Assessment Mode","text":"<p>This will work if the Airflow instances are on hosts accessible via SSH and SSH is configured to connect to all of these hosts.</p> <p>You can pass any configuration option that a Fabric Connection object can take.</p> <p>Create a file enumerating every host, such as: hosts.yaml<pre><code>ssh:\n  - host: airflow.foo1.bar.com\n  - host: root@airflow.foo2.bar.com\n  - host: airflow.foo3.bar.com\n    user: root\n    connect_kwargs: {\"key_filename\":\"/full/path/to/id_rsa\"}\n</code></pre> and run with: <pre><code>telescope -f hosts.yaml --organization-name &lt;My Organization&gt;\n</code></pre> This will produce a file ending in <code>*.data.json</code> - which is an data payload that can be sent to Astronomer for further processing and detailed analysis.</p>"},{"location":"#security-notice","title":"Security Notice","text":"<p>This project, by default, executes a python script downloaded from the internet on each Airflow it connects to. This is by design. Please fully understand this and take any steps required to protect your environment before running Telescope. Telescope <code>eval</code>'s any custom DAG obfuscation function passed to it.</p> <p>Artwork Orbiter logo by b farias used with permission from The Noun Project under Creative Commons.</p>"},{"location":"CLI/","title":"telescope","text":"<p>Telescope - A tool to observe distant (or local!) Airflow installations, and gather usage metadata</p> <p>Usage:</p> <pre><code>telescope [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --version                     Show the version and exit.\n  --local                       Airflow Reporting for local Airflow\n  --docker                      Autodiscovery and Airflow reporting for local\n                                Docker\n  --kubernetes                  Autodiscovery and Airflow reporting for\n                                Kubernetes\n  -l, --label-selector TEXT     Label selector for Kubernetes Autodiscovery\n                                [default: component=scheduler]\n  --dag-obfuscation             Obfuscate DAG IDs and filenames, keeping first\n                                and last 3 chars; my-dag-name =&gt; my-*****ame\n  --dag-obfuscation-fn TEXT     Obfuscate DAG IDs, defining a custom function\n                                that takes a string and returns a string;\n                                'lambda x: x[-5:]' would return only the last\n                                five letters of the DAG ID and fileloc\n  -f, --hosts-file PATH         Hosts file to pass in various types of hosts\n                                (ssh, kubernetes, docker) - See README.md for\n                                sample\n  -p, --parallelism INTEGER     How many cores to use for multiprocessing\n                                [default: (Number CPU)]\n  -n, --organization-name TEXT  Denote who this report belongs to, e.g. a\n                                company name\n  -o, --data-file PATH          Data file to write intermediate gathered data,\n                                can be '-' for stdout\n  -u, --presigned-url TEXT      URL to write data directly to - given by an\n                                Astronomer Representative\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"CLI/#presigned-url-upload","title":"Presigned URL Upload","text":"<p>You have the option to upload the data payload via a presigned upload url. Please contact an Astronomer Representative to acquire a presigned url.</p> <p>You can utilize this in the Telescope CLI as follows <pre><code>telescope --kubernetes --organization-name &lt;My Organization&gt; --presigned-url https://storage.googleapis.com/astronomer-telescope............c32f043eae2974d847541bbaa1618825a80ed80f58f0ba3\n</code></pre> Make sure to change <code>--kubernetes</code> to the correct method of operation to access your Airflow, and the contents of <code>--presigned-url</code> to the actual URL supplied to you.</p> <p>Note: Presigned URLs generally only last for up to a day, make sure to use yours soon after receiving it or request another when you are able.</p>"},{"location":"CLI/#configuration","title":"Configuration","text":""},{"location":"CLI/#local-autodiscovery","title":"Local autodiscovery","text":"<p>Either use <code>--local</code> or have an empty <code>local</code> key in your hosts file to enable autodiscovery. Autodiscovery simply runs the Airflow Report as a process, assuming that an Airflow Scheduler is being run on the current node.</p>"},{"location":"CLI/#docker-autodiscovery","title":"Docker autodiscovery","text":"<p>Either use <code>--docker</code> or have an empty <code>docker</code> key in your hosts file to enable autodiscovery. Autodiscovery searches for containers running locally that contain \"scheduler\" in the name and returns the container_id</p> <ul> <li><code>hosts.yaml</code> <pre><code>docker:\n</code></pre></li> </ul>"},{"location":"CLI/#kubernetes-autodiscovery","title":"Kubernetes autodiscovery","text":"<p>Either use <code>--kubernetes</code> or an empty <code>kubernetes</code> in your hosts file to enable autodiscovery. Autodiscovery searches for pods running in the Kubernetes cluster defined by <code>KUBEPROFILE</code> in any namespace, that contain the label <code>component=scheduler</code> (or another label defined by <code>--label-selector</code>), and returns the namespace, name, and container (<code>scheduler</code>)</p> <ul> <li><code>hosts.yaml</code> <pre><code>kubernetes:\n</code></pre></li> </ul>"},{"location":"CLI/#example-hostsyaml-input","title":"Example <code>hosts.yaml</code> input","text":"<p>use <code>-f hosts.yaml</code> <pre><code>local:\n\ndocker:\n  - container_id: demo9b25c0_scheduler_1\n\nkubernetes:\n  - namespace: astronomer-amateur-cosmos-2865\n    name: amateur-cosmos-2865-scheduler-bfcfbd7b5-dvqqr\n    container: scheduler\n\nssh:\n  - host: airflow.foo1.bar.com\n  - host: root@airflow.foo2.bar.com\n  - host: airflow.foo3.bar.com\n    user: root\n    connect_kwargs: {\"key_filename\":\"/full/path/to/id_rsa\"}\n</code></pre></p>"},{"location":"CLI/#label-selection","title":"Label Selection","text":"<p><code>--label-selector</code> allows Kubernetes Autodiscovery to locate Airflow Deployments with alternate key/values. The default is <code>component=scheduler</code>, however, if your Airflows contain <code>role=scheduler</code> instead, you would use <code>--label-selector \"role=scheduler\"</code>.</p>"},{"location":"CLI/#airflow-report-command","title":"Airflow Report Command","text":"<p><code>TELESCOPE_AIRFLOW_REPORT_CMD</code> can be set, normally the default is <pre><code>python -W ignore -c \"import runpy,os;from urllib.request import urlretrieve as u;a='airflow_report.pyz';u('https://github.com/astronomer/telescope/releases/latest/download/'+a,a);runpy.run_path(a);os.remove(a)\"\n</code></pre></p> <p>This can be used, for instance, if there is no access to Github on the remote box, or a custom directory is needed to run, or environment activation is required ahead of time.</p> <p>If your <code>python</code> is called something other than <code>python</code> (e.g. <code>python3</code>): <pre><code>TELESCOPE_AIRFLOW_REPORT_CMD=$(cat &lt;&lt;'EOF'\npython3 -W ignore -c \"import runpy,os;from urllib.request import urlretrieve as u;a='airflow_report.pyz';u('https://github.com/astronomer/telescope/releases/latest/download/airflow_report.pyz',a);runpy.run_path(a);os.remove(a)\"\nEOF\n) telescope -f hosts.yaml\n</code></pre></p> <p>or if you need to activate a <code>python</code> (such as with RedHat Linux) prior to running, and want to copy the telescope Manifest up to the host independently: <pre><code>scp airflow_report.pyz remote_user@remote_host:airflow_report.pyz\nTELESCOPE_AIRFLOW_REPORT_CMD=\"scl enable rh-python36 python -W ignore -c 'import runpy;a=\\'airflow_report.pyz\\';runpy.run_path(a);os.remove(a)'\" telescope -f hosts.yaml\n</code></pre></p>"},{"location":"CLI/#dag-obfuscation","title":"DAG Obfuscation","text":"<p><code>DAG ID</code> and <code>fileloc</code> can be obfuscated with the <code>--dag-obfuscation</code> command. The default obfuscation keeps the first 3 and last 3 characters and adds a fixed width of <code>******</code>. e.g. <pre><code>my-dag-name =&gt; my-*****ame\n</code></pre></p>"},{"location":"CLI/#custom-obfuscation-function","title":"Custom Obfuscation Function","text":"<p>If a different obfuscation function is desired, a <code>--dag-obfuscation-function</code> can be passed, which needs to be a python function that evaluates to <code>(str) -&gt; str</code>. E.g. <pre><code>--dag-obfuscation-fn=\"lambda x: x[-5:]\"\n</code></pre> would return only the last five letters of <code>dag_id</code> and <code>fileloc</code>. E.g. <pre><code>dag_id=\"hello_world\" -&gt; \"world\"\nfileloc=\"/a/b/c/d/filepath.py\" -&gt; \"th.py\"\n</code></pre></p>"},{"location":"CLI/#optional-environmental-variables","title":"Optional Environmental Variables","text":"<ul> <li><code>TELESCOPE_KUBERNETES_METHOD=kubectl</code> - to run with kubectl instead of the python SDK (often for compatibility reasons)</li> <li><code>TELESCOPE_REPORT_RELEASE_VERSION=x.y.z</code> - can be a separate telescope semver release number, to control which report gets run</li> <li><code>TELESCOPE_KUBERNETES_AIRGAPPED=true</code> - executes the airflow report in airgapped mode (i.e copies report binary from local to pod)</li> <li><code>LOG_LEVEL=DEBUG</code> - can be any support Python logging level <code>[CRITICAL, FATAL, ERROR, WARN, WARNING, INFO, DEBUG, NOTSET]</code></li> <li><code>TELESCOPE_SHOULD_VERIFY=false</code> - turn off helm chart collection - required to gather some data about Airflow in Kubernetes</li> <li><code>TELESCOPE_REPORT_PACKAGE_URL</code> - sets the URL that both the local CLI AND <code>TELESCOPE_AIRFLOW_REMOTE_CMD</code> will use (unless <code>TELESCOPE_AIRFLOW_REMOTE_CMD</code> is set directly)</li> </ul>"},{"location":"CLI/#compatibility-matrix","title":"Compatibility Matrix","text":"<p>Telescope is tested with</p> <p>Airflow versions <pre><code>    \"apache/airflow:slim-2.8.1\",\n    \"apache/airflow:slim-2.7.3\",\n    \"apache/airflow:slim-2.6.0\",\n    \"apache/airflow:slim-2.5.3\",\n    \"apache/airflow:slim-2.4.0\",\n    \"apache/airflow:2.3.4\",\n    \"apache/airflow:2.2.4\",\n    \"apache/airflow:2.1.3\",\n    \"apache/airflow:2.0.0\",\n    \"apache/airflow:1.10.15\",\n    \"apache/airflow:1.10.10\",\n    \"bitnami/airflow:1.10.2\",\n</code></pre></p> <p>Metadata Database Backends</p> <ul> <li>PostgreSQL</li> <li>SQLite</li> <li>MySQL (manual, infrequent testing)</li> <li>SQLServer (manual, infrequent testing)</li> </ul> <p>Python Versions <pre><code>        - \"3.7\"\n        - \"3.8\"\n        - \"3.9\"\n        - \"3.10\"\n</code></pre></p> <p>Operating Systems <pre><code>          - \"ubuntu-18.04\"\n          - \"windows-latest\"\n          - \"macos-latest\"\n</code></pre></p>"},{"location":"install/","title":"Installation &amp; Requirements","text":""},{"location":"install/#recommended-installation-method-1-via-binary","title":"Recommended Installation Method 1) via Binary","text":"<p>Find and download the executable in the Telescope Release for the correct version</p> <ul> <li> <p>for Linux (x86_64) <pre><code>wget https://github.com/astronomer/telescope/releases/latest/download/telescope-linux-x86_64\nchmod +x telescope-linux-x86_64\n</code></pre></p> </li> <li> <p>for Mac (x86_64, not M1 or ARM) <pre><code>wget https://github.com/astronomer/telescope/releases/latest/download/telescope-darwin-x86_64\nchmod +x telescope-darwin-x86_64\n</code></pre> Note: For Mac, you will get a Security error when you first run Telescope via the CLI binary - you can bypass this in <code>System Preferences -&gt; Security &amp; Privacy -&gt; General</code> and hitting <code>Allow</code></p> </li> <li> <p>for Windows (x86_64) <pre><code>wget https://github.com/astronomer/telescope/releases/latest/download/telescope-mingw64_nt-10.0-20348-x86_64.exe\nchmod +x telescope-mingw64_nt-10.0-20348-x86_64.exe\n</code></pre></p> </li> </ul>"},{"location":"install/#recommended-installation-method-2-via-pip","title":"Recommended Installation Method 2) via PIP","text":"<p>Note: PIP installation requires Python &gt;= 3.7</p> <p>optionally, create a virtualenv called <code>venv</code> (or anything else ) in the current directory for easy cleanup <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre></p> <p>Install Telescope using Pip from Github</p> <pre><code>python -m pip install astronomer-telescope\n</code></pre>"},{"location":"install/#requirements","title":"Requirements","text":""},{"location":"install/#local-pip-installation","title":"Local - pip installation","text":"<ul> <li>Python &gt;=3.7</li> <li><code>pip</code></li> </ul>"},{"location":"install/#local-binary-installation","title":"Local - binary installation","text":"<ul> <li>No requirements</li> </ul>"},{"location":"install/#local-assessment-mode-requirements","title":"Local - Assessment Mode Requirements","text":""},{"location":"install/#docker-permissions-to-exec-containers-dockersock-access-locally","title":"Docker Permissions to Exec Containers, <code>docker.sock</code> Access locally","text":""},{"location":"install/#kubernetes","title":"Kubernetes:","text":"<ul> <li>Permission to List Nodes and Exec in Pods</li> <li><code>KUBECONFIG</code> set locally</li> </ul>"},{"location":"install/#ssh","title":"SSH","text":"<ul> <li>Credentials to connect to all hosts</li> <li>SSH Access configured locally</li> </ul>"},{"location":"install/#local","title":"Local","text":"<ul> <li>Permission to execute Python locally</li> </ul>"},{"location":"install/#remote-airflow-requirements","title":"Remote Airflow Requirements","text":"<ul> <li>Airflow Scheduler &gt;1.10.5</li> <li>Python 3.x</li> <li><code>Postgresql</code>/<code>Mysql</code>/<code>Sqlite</code> Metadata Database (support not guaranteed for other backing databases)</li> <li><code>github.com</code> access (unless using )</li> <li><code>Kubernetes</code>: Kubernetes Scheduler has label <code>component=scheduler</code> (or <code>--label-selector</code> specified)</li> </ul>"},{"location":"install/#alternate-installation-methods","title":"Alternate Installation Methods","text":"<p>Telescope can also be installed as an Airflow plugin via Starship</p>"},{"location":"install/#install-from-source","title":"Install from Source","text":"<p>If neither the pip installation method nor binary installation methods work - you can download the source and execute directly as a python module</p>"},{"location":"install/#as-a-zip","title":"As a zip","text":"<pre><code>wget https://github.com/astronomer/telescope/archive/refs/heads/main.zip &amp;&amp; unzip main.zip\ncd telescope-main\npython -m telescope ...\n</code></pre>"},{"location":"install/#with-git","title":"With git","text":"<pre><code>git clone https://github.com/astronomer/telescope.git\ncd telescope\npython -m telescope ...\n</code></pre>"},{"location":"output/","title":"Data Collected &amp; Output","text":""},{"location":"output/#data-collected","title":"Data Collected","text":"<p>The following Data is collected:</p> <p>When run using <code>kubernetes</code></p> <ul> <li>cluster info is attained from the Nodes - including allocated and max CPU and Memory, number of nodes, and kubelet version</li> <li>Helm chart information for charts named like <code>astronomer</code> or <code>airflow</code> is fetched, sensitive values are redacted.</li> </ul>"},{"location":"output/#airflow-report","title":"<code>Airflow Report</code>","text":"<p>This information is saved under the <code>airflow_report</code> key, under the <code>host_type</code> key and the host key. E.g. <code>kubernetes.mynamespace|myhost-1234-xyz.airflow_report</code> or <code>ssh.my_hostname.airflow_report</code></p> <p>Using python <code>airflow_report.pyz</code> is downloaded and executed on the remote host (the host or container running the airflow scheduler). The performance impact of this report is negligible - <code>airflow.version.version</code> output to determine Airflow's version - <code>airflow.providers_manager.ProvidersManager</code>'s output, to determine what providers and versions are installed - <code>socket.gethostname()</code> to determine the hostname - <code>pkg_resources</code> to determine installed python packages and versions - <code>airflow.configuration.conf</code> to determine Airflow configuration settings and what is modified from defaults. Sensitive values are redacted - <code>os.environ</code> to determine what airflow settings, variables, and connections are set via ENV vars. Names only - the <code>pools</code> table is retrieved to list Airflow pools and sizes from the Airflow metadata db - the <code>dag</code> table is inspected from the Airflow metadata db   - <code>dags</code> are read off disk to attain variable and connection names, utilizing the filepath from the <code>dags</code> table - the <code>connection</code> table is fetched from the Airflow metadata db - the <code>variable</code> table is fetched from the Airflow metadata db - the <code>ab_user</code> table is fetched from the Airflow metadata db - the <code>task_instance</code> table is analyzed from the Airflow metadata db</p> <p>There is an intermediate output ending in <code>*.data.json</code> which contains all data gathered, and is utilized to generate the report outputs. The name of this file can vary depending on what options were passed to the tool.</p> <p>Output file includes the following sections:</p> Report Description airflow version report Airflow Deployment version configuration report Airflow runtime configuration (airflow.cfg) connections report List of all Airflow connections (IDs only) dags report Lisst of DAGs, including code quality metrics env vars report List of airflow-related environment variables hostname report Airflow Hostname configuration installed packages report List of all installed packages pools report List of Airflow pools and associated configuration providers report List of all installed providers usage stats report Execution statistics (success &amp; failure task counts) over the last 1, 7, 30, 365 days and all time. usage stats dag rollup report Execution statistics (success &amp; failure dag run counts) over the last 1, 7, 30, 365 days and all time. user report Number of active users over the last 1, 7, 30 and 365 days variables report List of all Airflow variables (keys only)"},{"location":"output/#airflow-version-report","title":"Airflow Version Report","text":"<p>Airflow Deployment version</p> <p>Example: <code>2.5.0+astro.1</code></p>"},{"location":"output/#configuration-report","title":"Configuration Report","text":"<p>Airflow runtime configuration (airflow.cfg)</p> <p>See documentation here See here for default airflow.cfg</p> Config Section Config Setting Example Value core dags_folder <code>/usr/local/airflow/dags</code> logging base_log_folder <code>/usr/local/airflow/logs</code> metrics statsd_on <code>True</code> secrets backend <code>***</code> cli endpoint_url <code>http://localhost:8080</code> debug fail_fast <code>False</code> api auth_backend <code>astronomer.flask_appbuilder.current_user_backend</code> lineage backend <code></code> operators default_owner <code>airflow</code> webserver base_url <code>https://deployments.astro.subdomain.domain.com/cluster/airflow</code> email email_backend <code>airflow.utils.email.send_email_smtp</code> smtp smtp_mail_from <code>noreply@domain.com</code> celery celery_app_name <code>airflow.executors.celery_executor</code> scheduler min_file_process_interval <code>90</code> ... ... ... <p>Note: Only one entry per config section is shown to reduce the length of the above table.</p>"},{"location":"output/#connections-report","title":"Connections Report","text":"<p>List of all Airflow connections (IDs only)</p> <p>Example:</p> Connection ID <code>airflow_db</code> <code>aws_default</code> <code>postgres_default</code> ..."},{"location":"output/#dags-report","title":"DAGs Report","text":"<p>List of DAGs, including code quality metrics</p> Field Name Description Example Value dag_id The id of the DAG <code>my_dag_id</code> schedule_interval The schedule dictating when the DAG runs are scheduled <code>0 1 * * *</code> root_dag_id The Parent DAG ID if dag is a SubDAG <code>null</code> is_paused If the DAG was paused (Boolean) <code>false</code> is_active If the DAG file is present in the DAGS_FOLDER <code>true</code> is_subdag If the DAG is defined within another DAG <code>false</code> fileloc Local Path to the DAG file <code>/usr/local/airflow/dags/my_dag_file.py</code> owners Name of the DAG owner <code>airflow</code> operators Comma-separated list of Operators used in DAG <code>\u00c8mptyOperator,PythonOperator</code> num_tasks Number of tasks in the DAG <code>4</code> variables Comma-separated list of variables referenced in the DAG <code>AIRFLOW_VAR_FOO_BAR</code> connections Comma-separated list of connections referenced in the DAG <code>AIRFLOW_CONN_AIRFLOW_DB</code> cc_rank Cyclomatic Complexity rating <code>\"A\"</code> mi_rank Maintainability Index score <code>\"A\"</code> analysis Subsection for Code Metrics Results from Radon analysis &gt; loc Total number of lines of code <code>55</code> analysis &gt; lloc Number of logical lines of code <code>15</code> analysis &gt; sloc Number of source lines of code <code>35</code> analysis &gt; comments Number of Python comment lines <code>3</code> analysis &gt; multi Number of lines representing multi-line strings <code>12</code> analysis &gt; blank Number of blank lines <code>15</code> analysis &gt; single_comments Number of blank lines (or whitespace-only ones) <code>3</code>"},{"location":"output/#environment-variables-report","title":"Environment Variables Report","text":"<p>List of airflow-related environment variables</p> <p>Example values: Note: Only the keys are fetched by Telescope for obvious security reasons.</p> <p>Example:</p> section Config config_options AIRFLOW__CORE__SQL_ALCHEMY_CONN connections AIRFLOW_CONN_AIRFLOW_DB variables AIRFLOW_VAR_FOO_BAR"},{"location":"output/#hostname-report","title":"Hostname Report","text":"<p>Airflow Hostname configuration</p> <p>Example: <code>astral-satellite-1234-scheduler-01abc23de-fghij</code></p>"},{"location":"output/#installed-packages-report","title":"Installed Packages Report","text":"<p>List of all installed packages</p> <p>Example:</p> Package Version ... ... apache-airflow 2.5.0+astro.1 apache-airflow-providers-amazon 6.2.0 apache-airflow-providers-apache-hive 5.0.0 apache-airflow-providers-apache-livy 3.2.0 apache-airflow-providers-celery 3.1.0 apache-airflow-providers-cncf-kubernetes 5.0.0 apache-airflow-providers-common-sql 1.3.1 apache-airflow-providers-databricks 4.0.0 apache-airflow-providers-dbt-cloud 2.3.0 apache-airflow-providers-elasticsearch 4.3.1 apache-airflow-providers-ftp 3.2.0 apache-airflow-providers-google 8.6.0 apache-airflow-providers-http 4.1.0 apache-airflow-providers-imap 3.1.0 apache-airflow-providers-microsoft-azure 5.0.1 apache-airflow-providers-postgres 5.3.1 apache-airflow-providers-redis 3.1.0 apache-airflow-providers-sftp 4.2.0 apache-airflow-providers-snowflake 4.0.2 apache-airflow-providers-sqlite 3.3.1 apache-airflow-providers-ssh 3.3.0 ... ..."},{"location":"output/#pools-report","title":"Pools Report","text":"<p>List of Airflow pools and associated configuration</p> <p>Example:</p> Pool Config Value default_pool total 100 default_pool running 0 default_pool queued 0 default_pool open 100"},{"location":"output/#providers-report","title":"Providers Report","text":"<p>List of all installed providers</p> <p>Example:</p> Package Version <code>apache-airflow-providers-amazon</code> <code>7.1.0</code> <code>apache-airflow-providers-google</code> <code>8.8.0</code> <code>apache-airflow-providers-microsoft-azure</code> <code>5.0.2</code> <code>apache-airflow-providers-postgres</code> <code>5.4.0</code> <code>apache-airflow-providers-slack</code> <code>7.2.0</code>"},{"location":"output/#usage-statistics-report","title":"Usage Statistics Report","text":"<p>Execution statistics (success &amp; failure task counts) over the last 1, 7, 30, 365 days, and all time.</p> Field Name Description Example Value dag_id The id of the DAG example_dag_basic 1_days_success Number of successful task runs in the last day. 4 1_days_failed Number of failed task runs in the last  day. 0 7_days_success Number of successful task runs in the last 7 days. 27 7_days_failed Number of failed task runs in the last 7 days. 1 30_days_success Number of successful task runs in the last 30 days. 108 30_days_failed Number of failed task runs in the last 30 days. 12 365_days_success Number of successful task runs in the last 365 days. 1430 365_days_failed Number of failed task runs in the last 365 days. 30 all_days_success Number of all time successful task runs. 1672 all_days_failed Number of all time failed task runs. 48"},{"location":"output/#usage-statistics-dag-rollup-report","title":"Usage Statistics DAG Rollup Report","text":"<p>Execution statistics (success &amp; failure dag run counts) over the last 1, 7, 30, 365 days, and all time.</p> Field Name Description Example Value dag_id The id of the dag example_dag_basic 1_days_success Number of successful dag runs in the last day. 2 1_days_failed Number of failed dag runs in the last  day. 0 7_days_success Number of successful dag runs in the last 7 days. 13 7_days_failed Number of failed dag runs in the last 7 days. 1 30_days_success Number of successful dag runs in the last 30 days. 33 30_days_failed Number of failed dag runs in the last 30 days. 6 365_days_success Number of successful dag runs in the last 365 days. 250 365_days_failed Number of failed dag runs in the last 365 days. 20 all_days_success Number of all time successful dag runs. 300 all_days_failed Number of all time failed dag runs. 24"},{"location":"output/#users-report","title":"Users Report","text":"<p>Number of active users over the last 1, 7, 30, and 365 days</p> Column Description Example Value 1_days_active_users Number of active users in the last day 2 7_days_active_users Number of active users in the last 7 days 4 30_days_active_users Number of active users in the last 30 days 8 365_days_active_users Number of active users in the last 365 days 9 total_users Total number of users 12"},{"location":"output/#variables-report","title":"Variables Report","text":"<p>List of all Airflow variables (keys only)</p> <p>Example:</p> Variable s3_bucket my_first_var my_second_var ..."}]}